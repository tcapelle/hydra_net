# AUTOGENERATED! DO NOT EDIT! File to edit: 04_trainer.ipynb (unless otherwise specified).

__all__ = ['Trainer']

# Cell
import wandb
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import DataLoader
from fastcore.all import *
from tqdm.notebook import tqdm
from accelerate import Accelerator

from .hydranet import HydraNet
from .data import NYUDataset

# Cell
class Trainer:
    "A simple trainer using Accelerate"
    def __init__(self, dataloader, model, loss_func, eval_dataloader=None, fp16=False, wandb=False):
        store_attr()
        self.optimizer = None
        self.scheduler = None
        self.acc = Accelerator(fp16=fp16)
        if wandb:
            wandb.init(project="HydraNet")

    def prepare(self):
        self.model, self.optimizer, self.dataloader = self.acc.prepare(self.model, self.optimizer, self.dataloader)

    def one_batch(self):
        return next(iter(self.dataloader))

    def train_step(self, inputs, targets):

        # Forward pass ➡
        outputs = self.model(inputs)
        loss = self.loss_func(outputs, targets)

        # Backward pass ⬅
        self.optimizer.zero_grad()
        self.acc.backward(loss)

        # Step with optimizer
        self.optimizer.step()
        if self.scheduler is not None:
            self.scheduler.step()

        return loss






    def train_log(self, loss, example_ct, pbar):
        # Where the magic happens
        if self.wandb:
            wandb.log({"train_loss": loss}, step=example_ct)
        pbar.set_postfix({"Loss" : f'{loss:.3f}'})

    def train_one_epoch(self):
        for b in (pbar :=tqdm(self.dataloader, leave=False)):
            images, depths, labels = b
            loss = self.train_step(images, depths).item()
            self.example_ct +=  len(images)
            self.train_log(loss, self.example_ct, pbar)
        return loss

    def eval_one_epoch(self, inputs, targets):
        self.model.eval()
        with torch.inference_mode():
            for b in tqdm(self.eval_dataloader, leave=False):
                # Forward pass ➡
                outputs = self.model(inputs)
                loss = self.loss_func(outputs, targets)

    def _fit(self, epochs=5):
        self.example_ct = 0
        self.prepare()
        for epoch in tqdm(range(epochs)):
            loss = self.train_one_epoch()
            print(loss)

    def fit(self, epochs=5, lr=1e-3):
        self.model.train()
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        self._fit(epochs)


    def fit_one_cyle(self, epochs=5, lr=1e-3, max_lr=1e-1):
        self.model.train()
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        self.scheduler = OneCycleLR(self.optimizer,
                                    max_lr=max_lr,
                                    steps_per_epoch=len(self.dataloader),
                                    epochs=epochs)
        self._fit(epochs)