# AUTOGENERATED! DO NOT EDIT! File to edit: 04_trainer.ipynb (unless otherwise specified).

__all__ = ['DEVICE', 'Trainer']

# Cell
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import OneCycleLR
from torch.utils.data import DataLoader
from fastcore.all import *
from tqdm.notebook import tqdm

from .hydranet import HydraNet
from .data import NYUDataset

# Cell
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Cell
class Trainer:

    def __init__(self, dataloader, model, loss_func, device=DEVICE):
        store_attr()
        self.optimizer = None
        self.scheduler = None
        self.model.to(device)


    def init_fit(self, lr=1e-4, max_lr=1e-1, epochs=10):
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        self.scheduler = OneCycleLR(self.optimizer, max_lr=max_lr, steps_per_epoch=len(self.dataloader), epochs=epochs)

    def one_batch(self):
        return next(iter(self.dataloader))

    def train_step(self, inputs, targets):
        "forward pass on one batch"

        inputs = tuplify(inputs.to(self.device))
        targets = targets.to(self.device)

        # Forward pass ➡
        outputs = self.model(*inputs)
        loss = self.loss_func(outputs, targets)
        print(loss)

        # Backward pass ⬅
        self.optimizer.zero_grad()
        loss.backward()

        # Step with optimizer
        self.optimizer.step()
        if self.scheduler is not None:
            scheduler.step()

        return loss

    def train_one_epoch(self, lr=1e-3):
        self.optimizer = Adam(self.model.parameters(), lr=lr)
        for b in tqdm(self.dataloader):
            images, depths, labels = b
            self.train_step(images, labels)

    # def fit(self, epochs=1):
    #     for epoch in tqdm(range(epochs)):
    #         for b in tqdm(self.dataloder,                loss = train_step(*b)